[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4063720.svg)](https://doi.org/10.5281/zenodo.4063720)

# F-UJI (FAIRsFAIR Research Data Object Assessment Service)
Developers:[Robert Huber](mailto:rhuber@marum.de), [Anusuriya Devaraju](mailto:anusuriya.devaraju@googlemail.com)

[![Publish Docker image](https://github.com/pangaea-data-publisher/fuji/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/pangaea-data-publisher/fuji/actions/workflows/publish-docker.yml)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4063720.svg)](https://doi.org/10.5281/zenodo.4063720)


## Overview

F-UJI is a web service to programatically assess FAIRness of research data objects based on [metrics](https://doi.org/10.5281/zenodo.3775793) developed by the [FAIRsFAIR](https://www.fairsfair.eu/) project. 
The service will be applied to demostrate the evaluation of objects in repositories selected for in-depth collaboration with the project.  

The '__F__' stands for FAIR (of course) and '__UJI__' means 'Test' in Malay. So __F-UJI__ is a FAIR testing tool.

**Cite as**

Devaraju, A. and Huber, R. (2021). An automated solution for measuring the progress toward FAIR research data. Patterns, vol 2(11), https://doi.org/10.1016/j.patter.2021.100370

### Clients and User Interface

A web demo using F-UJI is available at https://www.f-uji.net

An R client package that was generated from the F-UJI OpenAPI definition is available from https://github.com/NFDI4Chem/rfuji.

An open source web client for F-UJI is available is available at https://github.com/MaastrichtU-IDS/fairificator  

## Assessment Scope, Constraint and Limitation
The service is **in development** and its assessment depends on several factors. 
- In the FAIR ecosystem, FAIR assessment must go beyond the object itself. FAIR enabling services and repositories are vital to ensure that research data objects remain FAIR over time. Importantly, machine-readable services (e.g., registries) and documents (e.g., policies) are required to enable automated tests. 
- In addition to repository and services requirements, automated testing depends on clear machine assessable criteria. Some aspects (rich, plurality, accurate, relevant) specified in FAIR principles still require human mediation and interpretation. 
- The tests must focus on generally applicable data/metadata characteristics until domain/community-driven criteria have been agreed (e.g., appropriate schemas and required elements for usage/access control, etc.). For example, for some of the metrics (i.e., on I and R principles), the automated tests we proposed only inspect the ‘surface’ of criteria to be evaluated. Therefore, tests are designed in consideration of generic cross-domain metadata standards such as dublin core, dcat, datacite, schema.org, etc.
- FAIR assessment is performed based on aggregated metadata; this includes metadata embedded in the data (landing) page, metadata retrieved from a PID provider (e.g., Datacite content negotiation) and other services (e.g., re3data).

![alt text](https://github.com/pangaea-data-publisher/fuji/blob/master/fuji_server/static/main.png?raw=true)

## Requirements
Python 3.5.2+

### 308 redirects
In order to deal with 308 redirects, the following patch has to be applied on urrlib:
https://github.com/python/cpython/pull/19588/commits

### Google Dataset Search
* Download the latest Dataset Search corpus file from: https://www.kaggle.com/googleai/dataset-search-metadata-for-datasets
* Open file fuji_server/helper/create_google_cache_db.py and set variable 'google_file_location' according to the file location of the corpus file
* Run create_google_cache_db.py which creates a SQLite database in the data directory. From root directory run `python3 -m fuji_server.helper.create_google_cache_db`.

The service was generated by the [swagger-codegen](https://github.com/swagger-api/swagger-codegen) project. By using the
[OpenAPI-Spec](https://github.com/swagger-api/swagger-core/wiki) from a remote server, you can easily generate a server stub.  
The service uses the [Connexion](https://github.com/zalando/connexion) library on top of Flask.

## Usage
Before running the service, please set user details in the configuration file, see config/users.py.

To install F-UJI, you may execute the following python-based or docker-based installation commands from the root directory:

### Python module-based installation:

From the fuji source folder run
```bash
pip3 install .
```
or to install the last fixed dependencies
```
pip3 install .
```
The F-uji server can now be started with.
```
python3 -m fuji_server -c fuji_server/config/server.ini
```

### Docker-based installation:

```bash
docker run -d -p 1071:1071 ghcr.io/pangaea-data-publisher/fuji
```

To access the Swagger  user interface, open the url below on the browser:

```
http://localhost:1071/fuji/api/v1/ui/
```

Your Swagger definition lives here:

```
http://localhost:1071/fuji/api/v1/swagger.json
```

You can provide a different server config file this way:

```bash
docker run -d -p 1071:1071 -v server.ini:/usr/src/app/fuji_server/config/server.ini ghcr.io/pangaea-data-publisher/fuji
```

You can also build the docker image from the source code:

```bash
docker build -t <tag_name> .
docker run -d -p 1071:1071 <tag_name>
```

### How to run a simulation to the Swagger API
Firstly, un the file fuji_server/__main__.py by modifying the Run/Debug parameters with "-c config/server.ini".

Secondly, create the folder "examples/mass_assessment/results" if it does not exist; then run the script examples/mass_assessment/fuji_mass_eval_template.py
by specifying the DOI/URL of the dataset you want to test (variable "pids") and the rules you want to exclude ("rule_skip_list" of the
variable "base_request_dict").

The output will be a json file stored under the "results" folder. 

### How to add custom rules

This task requires much more steps than the previous one, let's see them below.

* Put the definition of the new rule in the file yaml/metrics_v0.5.yaml. As an example, I inserted the definition of rule FsF-F5-01D
with id equal to 18, with two practical tests with id FsF-F5-01D-1 and FsF-F5-01D-2. Metrics file used can be modified in server.ini file.

    **Note:** The definition of the new rule should follow the FAIR principles. As a result, the second part of the rule name 
    (in our example F5) should start with F or A or I or R. The last part of the rule name, instead, should end with D (Data), M (Metadata)
    or MD (both).


* Creation of three python files in models folder, for the definition of the rule. In particular, models/<rule_name>.py, 
models/<rule_name>_output.py, models/<rule_name>_output_inner.py

  * models/<rule_name>.py: file that contains the class definition of the rule, see models/test_rule.py as an example.
  * models/<rule_name>_output.py: file that contains the class definition of the rule output, see models/test_rule_output.py as an example.
  * models/<rule_name>_output_inner.py: file that contains the class definition of the rule output inner. In this class we can define
  some custom methods for the outputs of the rule. As an example, see the method 'prova' in models/test_rule_output_inner.py file.


* Put the implementation of the new rule in a new file called evalutators/fair_evalutator_<rule_name>.py. This class defines the implementation
of the rule, in particular the set of actions that the rule should follow during the evaluation. A simple case, in which the two
practical tests are passed without making any action is provided in evaluators/fair_evaluator_test_rule.py file.


* Definition of a check method in controllers/fair_check.py file. As an example, I defined a method called `check_test_rule_format(self)`, in which
there is the call to the method `FAIREvaluatorTestRule(self)` previously defined, that contains the implementation of our new custom rule.


* Insert the following lines in controllers/fair_object_controller.py:

    ```
    <rule_name>_result = ft.check_<rule_name>()
  
    results.append(<rule_name>_result)
    ```
  As an example, I added my new rule:

    ```
   # check method defined in the previous step
    test_rule_result = ft.check_test_rule_format()
  
    results.append(test_rule_result)
    ```
  
* Finally, edit the swagger file defined in yaml/swagger.yaml

  * In the structure FAIRResults/results/items/anyOf insert the following line: `- $ref: '#/components/schemas/<rule_name>'`.
  For our new custom rule the following line has been added: `- $ref: '#/components/schemas/TestRule'`. The path '#/components/schemas'
  refers to the fuji/models path.

  * After the FAIRResultCommon/ structure insert the following lines:

    ```
    <rule_name>:
      allOf:
        - $ref: '#/components/schemas/FAIRResultCommon'
        - type: object
          properties:
            output:
              $ref: '#/components/schemas/<rule_name>_output'
            test_debug:
              $ref: '#/components/schemas/Debug'
    ```

    For our new custom rule the following lines have been added:

    ```
    TestRule:
      allOf:
        - $ref: '#/components/schemas/FAIRResultCommon'
        - type: object
          properties:
            output:
              $ref: '#/components/schemas/TestRule_output'
            test_debug:
              $ref: '#/components/schemas/Debug'
    ```
    
  * After the FAIRResultCommon_score/ structure insert the following lines:

    ```
    <rule_name>_output:
      type: array
      items:
        $ref: '#/components/schemas/<rule_name>_output_inner'
    ```
      You can also define some properties, if needed. Have a look at the other rules for some suggestions.

      For our new custom rule the following lines have been added:

    ```
    TestRule_output:
      type: array
      items:
        $ref: '#/components/schemas/TestRule_output_inner'
    ```
  * After the body/ structure insert the following lines:

    ```
    <rule_name>_output_inner:
      type: object
      properties:
        <property_name>:
          type: <type_name>
    ```

    In the properties field it is necessary to add the methods defined in the models/<rule_name>_output_inner.py file.

    For our new custom rule, in which we had previously defined a method called 'prova' in the models/test_rule_output_inner.py
    file, the following lines have been added: 

    ```
    TestRule_output_inner:
      type: object
      properties:
        prova:
          type: string
    ```

### Notes

To avoid tika startup warning message, set environment variable TIKA_LOG_PATH. For more information, see [https://github.com/chrismattmann/tika-python](https://github.com/chrismattmann/tika-python)

If you receive the exception 'urllib2.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] on MacOS, run the install command shipped with Python :
./Install\ Certificates.command


## License
This project is licensed under the MIT License; for more details, see the [LICENSE](https://github.com/pangaea-data-publisher/fuji/blob/master/LICENSE) file.


## Acknowledgements

F-UJI is a result of the [FAIRsFAIR](https://www.fairsfair.eu/) “Fostering FAIR Data Practices In Europe” project which received funding from the European Union’s Horizon 2020 project call H2020-INFRAEOSC-2018-2020 (grant agreement 831558).

The project was also supported through our contributors by the [Helmholtz Metadata Collaboration (HMC)](https://www.helmholtz-metadaten.de/en), an incubator-platform of the Helmholtz Association within the framework of the Information and Data Science strategic initiative.
